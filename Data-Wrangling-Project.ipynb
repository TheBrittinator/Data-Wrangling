{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c2056a-e4ea-4337-82e7-51d8391dc207",
   "metadata": {},
   "source": [
    "# **Data Wrangling Project üßΩ‚ú®üìä**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e13c9c",
   "metadata": {},
   "source": [
    "# 1. Gather üë©üèΩ‚Äçüíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8e500-f00d-48b7-97a7-271b147ac7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TO DO\n",
    "- download zip file (from Kaggle or TheBrittinator git repository)\n",
    "- unzip the file using Python\n",
    "- import and extract CCSV file into pandas DataFrame\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# import the zipfile package and other packages for this project\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d7bea-5812-4781-af6f-d30abf382afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read zip file and extract all documents\n",
    "with zipfile.ZipFile('archive.zip', 'r') as myzip:\n",
    "    myzip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da69ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file into a pandas dataFrame\n",
    "\n",
    "df = pd.read_csv('online-job-postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59601a21-eb09-4a1c-9231-befc7ea2b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your work - using the .head method will only display the first 5 rows.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848392b",
   "metadata": {},
   "source": [
    "# 2. Assess ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f45d7-5c38-4cf4-a939-6340ed3ef90d",
   "metadata": {},
   "source": [
    "### **What are we assessing?**\n",
    "\n",
    "#### Not exploring our data just yet. First, let's assess our data's:\n",
    "1. Quality\n",
    "2. Tidiness\n",
    "\n",
    "# Quality\n",
    "### Common Data Quality Issues:\n",
    "- Missing data\n",
    "- Invlaid data\n",
    "- Inaccurate data\n",
    "- Inconsistent data\n",
    "\n",
    "*identify if your dataset checks any of these boxes.*\n",
    "\n",
    "# Tidiness\n",
    "### Also referred to as 'messy data.' This kind of data has issues with it's *structure:*\n",
    "Your data is **TIDY** if:\n",
    "- Each variable forms a column.\n",
    "- Each observation forms a row.\n",
    "- Each type of observational unit forms a table.\n",
    "\n",
    "# How to Assess Data\n",
    "### 2 styles to do it:\n",
    "1. Visually - with your eyes üëÄ\n",
    "2. Programmatic Assessment - using Pandas üêº\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68264a7-ce16-4173-a526-b3922628f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TO DO\n",
    "- Assess your dataset programmatically using pandas\n",
    "- identify data quality issues\n",
    "- confirm if your dataset is 'tidy' or 'messy.'\n",
    "\"\"\"\n",
    "\n",
    "# check programmatically! 4 methods we can use to do this\n",
    "## Display the first five rows of the DataFrame using .head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffb643-db80-4c86-ad78-52d5271fe914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five rows of the DataFrame using .tail\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae47a98-1eca-4771-a3f2-2c0dd6da121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a basic summary of the DataFrame using .info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd705f1-59eb-431c-a891-362cd4c1b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the entry counts for the Year column using .value_counts\n",
    "df[\"Year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bbbbf-ce86-45a7-9b2d-7ecdccc3a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check visually by reviewing your dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810394c-9f30-4842-8d1b-f51aa1a5262e",
   "metadata": {},
   "source": [
    "## What Did We Find?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba443b-b07a-42f4-8618-43554bf8ec22",
   "metadata": {},
   "source": [
    "1. Quality üèÜ\n",
    "    - Missing Records: lots of NaN values for some columns\n",
    "    - Multiple Terms that mean the same thing (in the 'start-date' column we found 'ASAP', 'immediately', 'as soon as possible')\n",
    "    - fix nondescriptive column headers such as 'RequiredQual' and typo 'jobrequirment'\n",
    "    \n",
    "2. Tidiness üèöÔ∏è\n",
    "    - does each variable have it's own column? ‚ùå\n",
    "    - does each observation have it's own row? ‚úÖ\n",
    "    - is each type of observational unit a table? ‚ùå\n",
    " * Both the date and Year + Month Column are representative of the same thing. This is a duplication. The day column needs to be added so that the entire date is seperated into columns. \n",
    " * There are two types of observational data: job posting and company data. These can be seperated into two tables.\n",
    "    \n",
    "#### **For this introductory lesson, we will focus on Data Quality in our cleaning step, NOT Tidiness.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c2aeb",
   "metadata": {},
   "source": [
    "# 3. Clean üßΩ‚ú®\n",
    "\n",
    "The prgrammatic Data Cleaning Process:\n",
    "1. Define - your data cleaning plan in writing. \n",
    "2. Code - transating these definitions into code and executing it.\n",
    "3. Test - testing our dataset, using code, to make sure our cleaning operations worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ab17c",
   "metadata": {},
   "source": [
    "### Define\n",
    "\n",
    "    - Select all records in the StartDate column that have \"As soon as possible\" \"immediately\" etc. and replace with \"ASAP\"\n",
    "    \n",
    "    - Select all nondescriptive and misspelled column headers (ApplicationP, AboutC, RequiredQual, JobRequirment) and replace then with full words (Application Procedure, AboutCompany, RequiredQualifications, JobRequirement)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eda0d4",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TO DO\n",
    "- copy your dataFrame for cleaning\n",
    "- rename your column headers \n",
    "    - https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas\n",
    "- find and replace redundancies in StartDate column with \"ASAP\"\n",
    "\"\"\"\n",
    "\n",
    "# copy our code into a new df - let's keep an original just in case.\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1131f97-0999-46c9-8d65-56b7dca74c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column headers - see docstring for resources\n",
    "df_clean = df.rename(columns={'ApplicationP':'ApplicationProcedure',\n",
    "                              'AboutC':'AboutCompany','RequiredQual':'RequiredQualifications',\n",
    "                              'JobRequirment':'JobRequirements'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fd885-0fa8-42e1-83eb-52260002d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and replace: locate our unique values\n",
    "df_clean.StartDate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75c938-c037-4d41-aa09-4ff0797830b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and replace: place all ASAP unique values into a variable write a for loop to replace each value with ASAP.\n",
    "# .replace structure: https://pandas.pydata.org/docs/reference/api/pandas.Series.replace.html\n",
    "# you can expand your value_counts and do the digging on your own, or just use this code:\n",
    "\n",
    "asap_list = ['Immediately', 'As soon as possible', 'Upon hiring',\n",
    "             'Immediate', 'Immediate employment', 'As soon as possible.', 'Immediate job opportunity',\n",
    "             '\"Immediate employment, after passing the interview.\"',\n",
    "             'ASAP preferred', 'Employment contract signature date',\n",
    "             'Immediate employment opportunity', 'Immidiately', 'ASA',\n",
    "             'Asap', '\"The position is open immediately but has a flexible start date depending on the candidates earliest availability.\"',\n",
    "             'Immediately upon agreement', '20 November 2014 or ASAP',\n",
    "             'immediately', 'Immediatelly',\n",
    "             '\"Immediately upon selection or no later than November 15, 2009.\"',\n",
    "             'Immediate job opening', 'Immediate hiring', 'Upon selection',\n",
    "             'As soon as practical', 'Immadiate', 'As soon as posible',\n",
    "             'Immediately with 2 months probation period',\n",
    "             '12 November 2012 or ASAP', 'Immediate employment after passing the interview',\n",
    "             'Immediately/ upon agreement', '01 September 2014 or ASAP',\n",
    "             'Immediately or as per agreement', 'as soon as possible',\n",
    "             'As soon as Possible', 'in the nearest future', 'immediate',\n",
    "             '01 April 2014 or ASAP', 'Immidiatly', 'Urgent',\n",
    "             'Immediate or earliest possible', 'Immediate hire',\n",
    "             'Earliest  possible', 'ASAP with 3 months probation period.',\n",
    "             'Immediate employment opportunity.', 'Immediate employment.',\n",
    "             'Immidietly', 'Imminent', 'September 2014 or ASAP', 'Imediately']\n",
    "\n",
    "for phrase in asap_list:\n",
    "    df_clean.StartDate.replace( phrase, 'ASAP', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996fe8f",
   "metadata": {},
   "source": [
    "### Test üë©üèΩ‚Äçüî¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ba63c-ce39-4b74-a9b6-58d7d95d80e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our work!\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45cfa2-557c-4ab1-b645-3b9c5bf4b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your work!\n",
    "df_clean.StartDate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that collapsed field makes the previous test hard to see. Let's create a loop that checks\n",
    "# for phrases in our asap_list in the df. for assert statements, 'no news is good news!':\n",
    "\n",
    "for phrase in asap_list:\n",
    "    assert phrase not in df_clean.StartDate.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9b86f-37ee-4255-9626-b190d421ef97",
   "metadata": {},
   "source": [
    "# So We've Done the Steps - Are We Finished?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d11c0-16ba-44b6-88f1-b79be39a36f2",
   "metadata": {},
   "source": [
    "You could be - but remember that data wrangling is an iterative process.\n",
    "You may need to\n",
    "\n",
    "- reassess after cleaning your data and clean further\n",
    "- gather more data\n",
    "\n",
    "That's okay! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce18eb-4525-48ec-9259-0193e2a6e3ed",
   "metadata": {},
   "source": [
    "# Bonus: Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd61f34-c9bb-4ba6-bd9b-f02b538c6e9d",
   "metadata": {},
   "source": [
    "### *What's the percentage of urgent job postings?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6dce6-fc4f-4db5-bdcd-06f37f03a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get our numerator - the number of ASAP posts:\n",
    "asap_counts = df_clean.StartDate.value_counts()['ASAP']\n",
    "asap_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed974e4-c327-4c53-88cd-76b6549e1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets get our denominator - total number of job postings:\n",
    "total_counts = df_clean.StartDate.count()\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e791c5c-49ce-4c22-b938-b51293aa61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of of urgent counts:\n",
    "asap_counts / total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8f35d-b905-46b0-ba37-9152ed1774fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a quick visualization:\n",
    "\n",
    "labels = np.full(len(df_clean.StartDate.value_counts()), \"\", dtype=object)\n",
    "labels[0] = 'ASAP'\n",
    "df_clean.StartDate.value_counts().plot(kind=\"pie\", labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
